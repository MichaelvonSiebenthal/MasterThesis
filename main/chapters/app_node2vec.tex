
  \section[Bank]{Node2Vec Results for the Bank Telemarketing Dataset}
  \label{app:n2v_bank}

  Appendix \ref{app:n2v_bank} contains the results using Node2Vec for the 
  bank telemarketing dataset are shown in table \ref{table:n2v_bank}. For the 
  downstream \acs{ml} 6-dimensional node embeddings were used.

  \begin{table}
    \centering
    \scalebox{1.0}{
    \begin{tabular}{|l||l|l|}
      \hline
      \textbf{ML Method} & \textbf{Training Accuracy} & \textbf{Validation
      Accuracy} \\
      \hline\hline
      Logistic Regression & 87.64\% & 89.95\% \\\hline 
      Support Vector Machine & 88.69\% & 87.62\% \\\hline
      ANN & 88.00\% & 89.72\% \\\hline
      Random Forest & 100\% & 87.40\% \\\hline
      AdaBoost & 89.52\% & 87.18\% \\\hline
      QDA & 86.06\% & 84.64\% \\
      \hline
    \end{tabular}}
    \caption{Node2Vec Classification Results Bank Telemarketing Dataset}
    \label{table:n2v_bank}
  \end{table}

  \noindent The models failed to overcome the challenge of unbalanced data for
  the training- and validation dataset. For that reason, Node2Vec was discarded
  for further analysis. 

  \section[Airline]{Node2Vec Results for the US Airline Passenger Dataset}
  \label{app:n2v5}

  Appendix \ref{app:n2v5} contains the results for Node2Vec using 5-dimensional 
  embeddings. Additional experiments with 15-dimensional embeddings further 
  revealed, that generating higher dimensional node embeddings did not improve 
  the accuracy of the downstream \acs{ml} models. In fact, it decreased the 
  model accuracies. To keep things simple, only the additional results using 
  5 embeddings are shown in table \ref{table:n2v5_res}.

  \begin{table}
    \centering
    \scalebox{1.0}{
    \begin{tabular}{|l||l|l|}
      \hline
      \textbf{ML Method} & \textbf{Training Accuracy} & \textbf{Validation
      Accuracy} \\
      \hline\hline
      Logistic Regression & 56.375\% & 59.75\% \\\hline 
      Support Vector Machine & 57.52\% & 55.17\% \\\hline
      ANN & 57.93\% & 56.08\% \\\hline
      Random Forest & 100\% & 54.08\% \\\hline
      AdaBoost & 60.54\% & 53.58\% \\\hline
      Naive Bayes & 57.52\% & 55.17\% \\\hline
      QDA & 57.46\% & 55.17\% \\
      \hline
    \end{tabular}}
    \caption{Node2Vec Classification Results with 5-Dimensional Embeddings}
    \label{table:n2v5_res}
  \end{table}

  \noindent The Node2Vec results which added the feature data to the
  2-dimensional node embeddings are presented in table \ref{table:n2v_feature}.

  \begin{table}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{|l||l|l|l|}
      \hline
      \textbf{ML Method} & \textbf{Training Accuracy} & \textbf{Validation
      Accuracy} & \textbf{Test Accuracy}\\
      \hline\hline
      Logistic Regression & 88.56\% & 86.00\% & 87.13\%  \\\hline 
      Support Vector Machine & 93.88\% & 94.08\% & 83.60\% \\\hline
      ANN & 96.04\% & 95.42\% & 84.50\% \\\hline
      Random Forest & 100\% & 93.83\% & 92.78\% \\\hline
      AdaBoost & 93.58\% & 92.50\% & 83.06\% \\\hline
      Naive Bayes & 85.88\% & 87.17\% & 82.67\% \\\hline
      QDA & 84.67\% & 85.08\% & 76.22\% \\
      \hline
    \end{tabular}}
    \caption{Node2Vec Classification Results with Feature Data}
    \label{table:n2v_feature}
  \end{table}

