  
  In this section the results for the US Airline Passenger Dataset are
  presented. First, the model specifications and results of graph
  representation learning and GNNs are presented. Afterwards, the graph machine
  learning results are compared to the results of "standard" machine
  learning methods.

  \section{Graph Representation Learning}

  The training graph consisting of 6'000 nodes which is a sub-sample of the
  US Airline Passenger training dataset provided on Kaggle \citep{KAGGLE2015}.
  2 Dimensional node representations were learned using the Node2Vec algorithm
  \citep{grover2016node2vec}. The model settings were defined as follows:

  \begin{itemize}[itemsep=1mm]
    \item Dimensions: 2
    \item Random walk length: 8
    \item Number of random walks: 100
    \item Window size: 10
    \item Node batch size: 2
    \item Return parameter $p=1$
    \item In-out parameter $q=1$
  \end{itemize}

  \noindent With the specified Return- and In-out parameter, the Node2Vec
  output corresponds to the DeepWalk output. The resulting node embeddings were
  then used as inputs for standard machine learning methods. An overview of the
  node embeddings is shown in figure \ref{fig:node2vec}. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.7\textwidth]{node2vec_emb.png}
		\caption{Node2Vec embeddings}
        \label{fig:node2vec}
  \end{figure}

  \noindent The embeddings reveal very interesting insights in term of cluster
  analysis. First, the observation seen in figure \ref{fig:us_airline_graph}
  mainly correspond to the observations in figure \ref{fig:node2vec}. The
  advantage of graph representation learning, is that the nodes plotted using
  Node2Vec corresponds to a proper scatter plot in Euclidean space. In
  addition, the nodes are group in a much nicer and more orderly fashion. This
  makes it possible to infer with a higher level of certainty that for instance
  business class passengers traveling for business purposes tend to be more
  satisfied than other passengers on average. The 2 dimensional node embeddings
  thus provide interesting insights for cluster analysis. \\

  \noindent The node embeddings were used for three standard machine learning
  methods which are presented in table \ref{table:node2vec_results}.

  \begin{table}[h]
    \centering
    \begin{tabular}{|l||l|l|}
      \hline
      \textbf{ML Method} & \textbf{Training Accuracy} & \textbf{Validation
      Accuracy}\\
      \hline\hline
      Logistic Regression & 76.85\% & 75.83\% \\\hline 
      Support Vector Machine & 76.95\% & 75.66\% \\\hline
      ANN & 77.61\% & 76.58\% \\
      \hline
    \end{tabular}
    \caption{Node2Vec Classification Results}
    \label{table:node2vec_results}
  \end{table}

  \noindent The results show, that Node2Vec only has very modest success in
  classifying passengers according to their satisfaction. When looking at the
  satisfaction scatter plot in figure \ref{fig:node2vec} it becomes obvious why
  the downstream machine learning tasks only had limited success. For the
  variables which were used for generating the graph and for which neighborhood
  clusters emerged, Node2Vec created very good node embeddings. The label
  variable satisfaction could not be used for the graph generation as this would
  be unrealistic. Since the label is not considered for the graph generation,
  Node2Vec does not create embeddings which consider the label. The label is
  only considered to the extend that the variables used for the graph
  generation process create structures which are related somehow to the label.
  For that reason, the success of any downstream machine learning method will
  be limited to the extent that the node embeddings capture relevant information
  for predicting the label. The Python code for generating the node embeddings 
  and the subsequent machine learning methods can be found in appendix X. 

  \section{Graph Neural Networks}

  This subsection will present the results using graph neural networks. As
  mentioned in section 2, GCN by \cite{kipf2016semi} and
  GraphSage by \cite{hamilton2017inductive} will be used for classifying the
  satisfaction of the US airline passenger dataset. The model specifications 
  and results are presented in the following subsections.

  \subsection{Graph Convolutional Network}

  The GCN was designed using the exact forward propagation function
  previously outlined in equation \ref{eq:GCN_forward}.

  \begin{equation}
	  Z = f(X,A) = \text{softmax}\left(\hat A \;\text{ReLu}\left(\hat A X
	  W^{(0)}\right)W^{(1)}\right)
      \label{eq:GCN_forward_1}
  \end{equation}

  \noindent The node features $X$ correspond to the 21 selected explanatory 
  variables of the US airline passenger dataset as described in section 3. The
  node features were standardized due to large differences between the
  variables in terms of scale. In addition, standardizing yielded significant
  performance improvements. The hidden layer size for both convolutional layers
  was set to 21 and the output layer was set to size 2 for the binary
  classification. The training loss was calculated using cross-entropy loss and 
  the model parameters were updated using the Adam optimizer
  \citep{kingma2014adam} with the learning rate set to $0.002$. Essentially,
  algorithm \ref{algo:GNN_struct} ca be applied by replacing the forward
  propagation with equation \ref{eq:GCN_forward_1} and the model parameters are
  updated with the Adam optimizer instead of standard gradient descent.
  Different model specification were tested, however the chosen specifications
  appeared to perform very well. For example, choosing a hidden layer size of
  100 or 128 yielded virtually identical results. A stronger reduction in
  hidden layer size however reduces the performance of the GCN. Adding additional
  convolutional layers decreased the improvements and decreasing the learning
  rate decreased the performance. This graph convolutional network requires
  approximately 1'200 epochs to finish training. The loss- and accuracy plots
  are shown in figure \ref{fig:gcn_plots}.

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{gcn_plots.png}
		\caption{GCN Loss- and Accuracy Plots}
        \label{fig:gcn_plots}
  \end{figure}

  \noindent The results are are further presented in table
  \ref{table:gcn_results_train} \& \ref{table:gcn_results_valid}. 

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 893 & 143 \\\hline 
      \textbf{Satisfied} & 148 & 605 \\\hline\hline
      \textbf{Accuracy} & 83.73\% & \\
      \hline
    \end{tabular}
    \caption{Confusion Matrix Training Data}
    \label{table:gcn_results_train}
  \end{table}

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 2'004 & 383 \\\hline 
      \textbf{Satisfied} & 339 & 1'485 \\\hline\hline
      \textbf{Accuracy} & 82.85\% & \\
      \hline
    \end{tabular}
    \caption{Confusion Matrix Validation Data}
    \label{table:gcn_results_valid}
  \end{table}

  \noindent Graph convolutional networks are designed to be used in a
  transductive setting and the model cannot be applied to new and unseen
  graphs. In this setting 30\% of the dataset was used for training and 70\% of
  the data was used for validation. This is done by masking the nodes in the
  graph which are part of the validations dataset. The masked nodes are 
  considered in the in the neighborhood function $\mathcal{N}(v)$, they are 
  however not considered as target nodes. Therefore, masked nodes are not use
  for updating the model parameters. The fact, that GCNs cannot be used in an
  inductive setting is very limiting. Further, the GCN requires over 1'200
  epochs to start finish training and yields only mediocre results in terms of
  accuracy and model fit. A reason for this could be that only a full-batch
  implementation for GCN was introduced. In addition, GCNs always consider the
  entire neighborhood set. Sampling the neighborhood and mini-batch training 
  which is an important ingredient of GraphSage and is introduced as an
  improved and superior GNN method in the following subsection.

  \subsection{GraphSage}

  For GraphSage the exact same data input was used as for the GCN. The 
  GraphSage model included 2 convolutional layers where both have a hidden 
  layer size of 128 and the output layer is of size 2. The model is defined
  using an adaptation of algorithm \ref{algo:GraphSage}. Similar as for the 
  GCN, the training graph is split into 80\% training and 20\% validation using 
  node masking. The loss is calculated using cross-entropy and the model 
  parameters are updated using the Adam optimizer with the learning rate set to 
  0.002 and is an adaptation to the model updating procedure shown in 
  algorithm \ref{algo:GNN_struct}. The output of the first convolutional layer 
  is activated using the ReLu function and the output of the second 
  convolutional layer is activated using the log-softmax function. The model was 
  trained using a mini-batch size of 50 nodes and the neighborhood function 
  $\mathcal{N}(v)$ randomly sampled 10 neighbors 2-hops away from the target
  node and randomly sampled 5 nodes at a 1-hop distance from the target node. 
  This corresponds to the steps shown in figure \ref{fig:GraphSage_sample}. 
  To improve the robustness of the model, lastly a dropout rate of $p = 0.02$ 
  was set. GraphSage was run using the different aggregation strategies mean, 
  LSTM, max-pooling. In addition, sum-pooling was added as an additional 
  aggregation strategy as suggested by \cite{xu2018powerful}. Sum-pooling 
  aggregation follows the same procedure as max-pooling with the only difference 
  being that element-wise sum-pooling is performed and not max-pooling. This 
  aggregation method has been shown to be more robust for graph neural networks, 
  as it allows the GNN to better distinguish different graph structures. It is
  important to note, that the Graphsage sum-pooling strategy employed does not
  correspond to the Graph Isomorphism Network introduced by 
  \cite{xu2018powerful}. This article rather provided the inspiration for
  applying sum-pooling for Graphsage. \\
  
  \noindent The model for each aggregation strategy was trained using 400 epochs. 
  The training- and validation results using the training graph are presented 
  for every aggregation strategy. In addition, the trained models are applied 
  to a new unseen graph also consisting of 6'000 nodes to check for the 
  inductive capability of the Graphsage model. 

  \paragraph{Mean Aggregation}  \mbox{}\\ 
  In figure \ref{fig:mean_aggregation} the training- and validation loss as
  well as the accuracies for the mean aggregation model is shown.

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_mean_plots.png}
		\caption{Mean Aggregation Loss- and Accuracy Plots}
        \label{fig:mean_aggregation}
  \end{figure}

  \noindent The training accuracy is 94.98\% and the validation accuracy is 
  94.59\% after 400 epochs. The model resulted in a test accuracy of 93.38\%
  with the following confusion matrix shown in table
  \ref{table:mean_results_test}:


  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'262  & 94 \\\hline 
      \textbf{Satisfied} & 303 & 2'341 \\\hline\hline
      \textbf{Accuracy} & 93.38\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix Mean Aggregation}
    \label{table:mean_results_test}
  \end{table}

  \paragraph{LSTM Aggregation}  \mbox{}\\ 
  Figure \ref{fig:lstm_aggregation} shows the training- and validation loss
  and accuracy using LSTM aggregation. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_lstm_plots.png}
		\caption{LSTM Aggregation Loss- and Accuracy Plots}
        \label{fig:lstm_aggregation}
  \end{figure}

  \noindent The training- and validation accuracy after 400 epochs was 94.44\% 
  and 94.09\% respectively. Here again, we can see that the training behavior
  is relatively good with a small over-fit which remains stable. The results
  for the test graph are shown in table \ref{table:lstm_results_test}.

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'259  & 97 \\\hline 
      \textbf{Satisfied} & 295 & 2'349 \\\hline\hline
      \textbf{Accuracy} & 93.47\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix LSTM Aggregation}
    \label{table:lstm_results_test}
  \end{table}

  \paragraph{Sum-Pooling Aggregation}  \mbox{}\\ 
  Sum-pooling is shown to provide more consistent results as it prevents the
  GNN from being confused \citep{xu2018powerful}. This aggregation method 
  provides solid results as shown in figure \ref{fig:sum_aggregation}. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_sum_plots.png}
		\caption{Sum-Pooling Aggregation Loss- and Accuracy Plots}
        \label{fig:sum_aggregation}
  \end{figure}

  \noindent The plots show that sum-pooling trains very well. The training 
  accuracy is 94.33\% and validation accuracy is 94.2\% after the model
  finished training. The results for the test graph are shown in table
  \ref{table:sum_results_test}.

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'218  & 138 \\\hline 
      \textbf{Satisfied} & 212 & 2'432 \\\hline\hline
      \textbf{Accuracy} & 94.17\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix Sum-Pooling}
    \label{table:sum_results_test}
  \end{table}

  \paragraph{Max-Pooling Aggregation}  \mbox{}\\ 
  The last aggregation method is max-pooling which yielded similar results as
  sum-pooling. The training- and validation loss as well as the accuracies are 
  shown in figure \ref{fig:max_aggregation}. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_max_plots.png}
		\caption{Max-Pooling Aggregation Loss- and Accuracy Plots}
        \label{fig:max_aggregation}
  \end{figure}

  \noindent The training model resulted with a validation accuracy
  of 93.96\% and training accuracy of 93.84\%. The results for the test graph
  are shown in table \ref{table:max_results_test}.

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'180  & 176 \\\hline 
      \textbf{Satisfied} & 196 & 2'448 \\\hline\hline
      \textbf{Accuracy} & 93.8\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix Max-Pooling}
    \label{table:max_results_test}
  \end{table}

  \subsection{GraphSage Robustness Simulation}
  
  The loss- and accuracy plots reveal mostly a relatively good model fit. LSTM
  aggregation however shows a moderate over-fit and is indicative of a more 
  general issue shown to be true for all aggregation strategies. Simulations
  revealed, that for the same graph the trained model would either over-fit,
  fit perfectly or have a training pattern in which the validation loss was
  smaller than the training loss. An analysis revealed, that the culprit for
  this behavior was the dropout rate of 2\% as well as the random assignment of
  graph nodes into training- and validation data. If the dropout rate was set
  to 0, the model always tended to over-fit, where the training accuracy would
  approach 100\% and the validation accuracy would stagnate around 93-94\%.
  Unfortunately, when applying this model to an unseen test graph, the accuracy
  results were somewhat lower at 91-92\%. For this reason, a dropout rate at
  2\% was set to avoid this over-fitting problem which in turn yielded better
  test results. This has the consequence, that it is more difficult for the
  model to make predictions for the training data compared to the validation
  data which uses no dropout rate. This obstacle however has the downside, that
  it makes the model sensitive to the node assignments during the random
  train- and validation split. Some nodes are more difficult to learn depending
  on their features and neighbors. This means, that if the training set
  includes more difficult nodes on average than the validation set in
  conjunction with the dropout rate as an additional obstacle for the training
  set, the validation loss becomes lower than the training loss. The same is
  true in reverse, which leads to an over-fit. Lastly, the if the nodes in both
  the training- and validation set are on average of similar difficulty a good
  model fit is achieved as shown in the loss- and accuracy plots. \\

  \noindent Nevertheless, the training- and validation results shown are 
  representative for the model in general. In order to show this, a simulation 
  was run using a training- and validation graph consisting of 6'000 nodes and 
  a test graph also consisting of 6'000 nodes. For the simulation, max-pooling 
  aggregation was selected using the same model specifications as previously. 
  The model was trained during 100 experiments, where for every experiment a 
  new random training- and validation split node assignment was performed. The
  trained model was then applied to the test graph where the loss- and accuracy
  was measured. The average results of the 100 experiments are shown in table
  \ref{table:simulation_results} and figure \ref{fig:simulation_results}. 

  \begin{table}[h]
    \centering
      \begin{tabular}{|l||c|c|c|}
      \hline
      \textbf{Metric} & \textbf{Training Set} & \textbf{Validation Set} & 
      \textbf{Test Set}\\
      \hline\hline
      Average Accuracy & 94.37\% & 94.23\% & 93.21\% \\\hline 
      Average Cross-Entropy Loss & 0.3684 & 0.3693 & 0.3792 \\
      \hline
      \end{tabular}
    \caption{Average Simulation Results}
    \label{table:simulation_results}
  \end{table}

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{simulation_results.png}
		\caption{Simulation Results Max-Pooling}
        \label{fig:simulation_results}
  \end{figure}

  \noindent Max-pooling was selected as this corresponds to the recommended
  aggregation strategy \citep[p. 9]{hamilton2017inductive}. For the remaining
  aggregation strategies, only 10 experiments were run due to time
  considerations as 100 experiments require approximately 15 hours to complete.
  The results using only 10 experiments yielded similar results, with test
  accuracies ranging predominantly from 93-94\%. 

  \subsection{Result Comparison}

  The results shown for the three models Node2Vec, GCN and GraphSage are
  compared to the results using "standard" machine learning methods. This is
  done to assess to what extent, synthetic graph generation is a useful
  approach for machine learning. This comparison is an important ingredient for
  answering the research question. For the comparison, the methods logistic
  regression \citep{cramer2002origins}, naive bayes \citep{zhang2004bayes},
  support vector machines (SVM) \citep{platt1999probabilistic,chang2011libsvm},
  Random Forest Classifier \citep{breiman2001random}, AdaBoost Classifier
  \citep{freund1997decision,hastie2009multi} and an Artificial Neural Network
  \citep{mcculloch1943logical} are used. These methods only consider the
  feature data and do not consider any network connections. These methods are
  therefore simpler and do not require a graph to be generated. A comparison of
  the different results is shown in table \ref{table:result_comparison}.

  \begin{table}[h]
    \centering
    \scalebox{0.75}{
      \begin{tabular}{|l||c|c|c|}
      \hline
      \textbf{Method} & \textbf{Training Accuracy} & \textbf{Validation
      Accuracy} & \textbf{Test Accuracy}\\
      \hline\hline
      Logistic Regression & 88.16\% & 87.08\% & 86.43\% \\\hline 
      Naive Bayes & 87.46\% & 85.83\% & 85.6\% \\\hline
      AdaBoost & 93.54\% & 93.08\% & 92.45\% \\\hline
      Random Forest & 100\% & 95.08\% & 94.38\% \\\hline
      SVM & 94.42\% & 93.5\% & 92.45\% \\\hline
      ANN & 96.47\% & 94.67\% & 93.32\%\\\hline
      Node2Vec & 77.61\% & 76.58\% & -\\\hline
      GCN & 83.73\% & 82.85\% & -\\\hline
      GraphSage (Mean Aggregation) & 94.98\% & 94.59\% & 93.38\% \\\hline
      GraphSage (LSTM Aggregation) & 94.44\% & 94.09\% & 93.47\% \\\hline
      GraphSage (Sum-Pooling) & 94.33\% & 94.2\% & 94.17\% \\\hline
      GraphSage (Max-Pooling) & 93.96\% & 93.84\% & 93.8\% \\
      \hline
      \end{tabular}}
    \caption{Result Comparison}
    \label{table:result_comparison}
  \end{table}

  \noindent Table \ref{table:result_comparison} shows, that GCN and Node2Vec
  are not competitive when using MAG generated graphs. GraphSage is however a 
  serious competitor and appears to be the second best method overall. The
  Random Forest classifier is consistently the best method regardless of the
  training- and validation data as well as the test data used. The GraphSage
  models are consistently shown to be the second best method. The ANN model
  performs similarly well in terms of achieving a similar test accuracy.
  Similar as with the GNNs, it is important to evaluate the model fit to draw a
  more definitive conclusion which is shown in figure \ref{fig:ANN_fit}.

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{ANN_Plot.png}
		\caption{ANN Model Fit}
        \label{fig:ANN_fit}
  \end{figure}

  \noindent Figure \ref{fig:ANN_fit} shows, that the ANN model has a relatively
  strong over-fit compared to the GNN models. This observation is consistent
  for every ANN model trained. In addition, different number of hidden layers, 
  hidden layer sizes and dropout rates were tested, which all resulted in a 
  clear over-fit or bad training behavior. For that reason, the ANN is deemed
  to be inferior to the GNN, even if the test accuracies are very similar. \\

  \noindent AdaBoost and SVM also are shown to provide very good results,
  however yield slightly lower test accuracies compared to the previous
  methods. Lastly, naive bayes and logistic regression clearly yield inferior
  results. 
