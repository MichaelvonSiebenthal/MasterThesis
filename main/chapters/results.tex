  
  In this chapter the results for the US Airline Passenger Dataset are
  presented. First, the model specifications and results of graph
  representation learning and GNNs are presented. Afterwards, the graph machine
  learning results are compared to the results using "standard" machine
  learning methods. For the comparison, the methods logistic
  regression \citep{cramer2002origins}, naive bayes \citep{zhang2004bayes},
  support vector machines (SVM) \citep{platt1999probabilistic,chang2011libsvm},
  Random Forest Classifier \citep{breiman2001random}, AdaBoost Classifier
  \citep{freund1997decision,hastie2009multi}, Quadratic Discriminant Analysis
  (QDA) \citep{tharwat2016linear} and Artificial Neural Networks
  \citep{mcculloch1943logical} are considered.

  \section{Graph Representation Learning}

  The graph generated in section \ref{section:graph_gen} using the MAG method
  was used for graph representation learning. The Node2Vec algorithm using an
  unbiased walk was employed for learning the 2-dimensional node representations 
  of the graph. The Node2Vec algorithm was employed using following model 
  parameters:

  \begin{itemize}
    \setlength\itemsep{0.1em}
    \item Embedding size $d$: 2
    \item Random walk length $t$: 8
    \item Number of random walks $\gamma$: 100
    \item Window size $w$: 10
    \item Node batch size: 2
    \item Return parameter $p=1$
    \item In-out parameter $q=1$
  \end{itemize}

  \noindent With the specified return- and in-out parameters, the Node2Vec
  output corresponds to the DeepWalk output. The resulting node embeddings were
  then used as inputs for standard machine learning methods. The resulting node 
  embeddings are shown in figure \ref{fig:node2vec}. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{node2vec_emb.png}
		\caption{Node2Vec embeddings}
        \label{fig:node2vec}
  \end{figure}

  \noindent The plotted node embeddings in figure \ref{fig:node2vec} reveal 
  interesting neighborhood structures. First, the nodes are split according to
  their type of travel. This corresponds to the two main clusters shown in
  figure \ref{fig:us_airline_graph}. Secondly, the node embeddings are grouped 
  in a nice and orderly fashion. The node embeddings are part of Euclidean
  space, which is why the plots shown in figure \ref{fig:node2vec} are proper
  scatter plots. This allows for direct comparison of nodes and the groups
  which they belong to. The 2-dimensional node embeddings are thus useful for
  gaining insights via data visualization.\\

  \noindent The node embeddings were used as input data for the three standard 
  machine learning methods shown in table \ref{table:node2vec_results}.

  \begin{table}[h]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{|l||l|l|l|}
      \hline
      \textbf{ML Method} & \textbf{Training Accuracy} & \textbf{Validation
      Accuracy} & \textbf{Test Accuracy}\\
      \hline\hline
      Logistic Regression & 77.04\% & 76.16\% & 57.05\%  \\\hline 
      Support Vector Machine & 76.71\% & 77.00\% & 39.08\% \\\hline
      ANN & 76.78\% & 78.08\% & 29.48\% \\\hline
      Random Forest & 100\% & 73.08\% & 40.63\% \\\hline
      AdaBoost & 76.96\% & 77.00\% & 58.22\% \\\hline
      Naive Bayes & 75.73\% & 76.43\% & 56.97\% \\\hline
      QDA & 75.52\% & 77.33\% & 57.00\% \\
      \hline
    \end{tabular}}
    \caption{Node2Vec Classification Results}
    \label{table:node2vec_results}
  \end{table}

  \noindent The results show, that Node2Vec model was not successful for
  classifying passengers according to their satisfaction. Applying the trained
  models to the test data yielded even poorer results. When looking at the
  satisfaction scatter plot in figure \ref{fig:node2vec}, it becomes obvious why
  the downstream machine learning tasks were unsuccessful. Node2Vec generated
  very good node embeddings for the attributes to the extent that clusters exist
  within the original graph. The label satisfaction could not be used as an 
  attribute for generating the graph, as this would be unrealistic in practice. 
  As the label is not considered for the graph generation process, Node2Vec 
  does not create embeddings which directly consider the label. The label is 
  only considered to the extend that the attributes create structures 
  which are related with the label. For that reason, the success of any 
  downstream machine learning method will be limited to the extent that the node 
  embeddings capture relevant information for predicting the label. \\

  \noindent Alternative model specifications were tested, which mainly included
  learning higher dimensional node embeddings. These node embeddings however
  did not yield better results. Given the almost identical accuracies, the more
  parsimonious model with 2 dimensions is preferred. As a final test, the node
  embeddings were joined to the feature data presented in section 
  \ref{section:airline_data}. This data was then used as the input data for the
  downstream machine learning models. For the training- and validation data,
  this approach yielded excellent results with accuracies often being close to 
  95\%. For some of the models such as the Random Forest classifier, this also
  translated into good accuracies for the test data. Unfortunately, the results 
  presented in section \ref{section:result_comp} show, that better accuracies 
  are achieved only using the feature data. For that reason, joining feature 
  data with node embeddings is not a recommended approach for the US Airline 
  Passenger dataset.

  \section{Graph Neural Networks}

  This section presents the results using graph neural networks. As
  mentioned in section \ref{section:GNN_theory}, GCN by \cite{kipf2016semi} and
  GraphSage by \cite{hamilton2017inductive} are used for classifying the
  satisfaction of the US airline passengers. The model specifications 
  and results are presented in the following sections for both methods.

  \subsection{Graph Convolutional Network}

  The GCN was designed using a similar forward propagation function as the
  function shown in equation \ref{eq:GCN_forward}. The only difference is, that
  the output layer is activated using the logSoftmax function instead of the
  Softmax function. The outputs of both activation functions are theoretically 
  the same. The logSoftmax function is however numerically more stable, as it 
  internally makes use of the log-sum-exp trick. A good explanation of this
  trick is given on the website by \cite{gundersen2020}\footnote{Website 
  Gregory Gundersen: \\\url{https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/}}.

  \begin{equation}
	  Z = f(X,A) = \text{logSoftmax}\left(\hat A \;\text{ReLU}\left(\hat A X
	  W^{(0)}\right)W^{(1)}\right)
      \label{eq:GCN_forward_1}
  \end{equation}

  \noindent The node features $X$ include to the 21 explanatory variables of the 
  US airline passenger dataset as described in section 
  \ref{section:airline_data}. The categorical variables were one-hot encoded, 
  which is why the feature matrix includes 24 variables. The hidden layer size 
  of the convolutional layers was also set to 24 and the output layer was set 
  to size 2 for the binary classification task. The training loss was 
  calculated using cross-entropy loss and the model parameters were updated 
  using the Adam optimizer \citep{kingma2014adam} with the learning rate set to 
  $0.002$. Essentially, algorithm \ref{algo:GNN_struct} ca be applied by replacing the 
  forward propagation with equation \ref{eq:GCN_forward_1} and updating the 
  model parameters using the Adam optimizer instead of standard gradient descent.
  Different model specifications were tested, for which the chosen specifications
  performed best. The GCN required approximately 1'000 epochs to finish training. 
  The resulting loss- and accuracy plots are shown in figure \ref{fig:gcn_plots}.

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{gcn_plots.png}
		\caption{GCN Loss- and Accuracy Plots}
        \label{fig:gcn_plots}
  \end{figure}

  \noindent The resulting confusion matrices and the final accuracies are 
  shown in tables \ref{table:gcn_results_train} \& 
  \ref{table:gcn_results_valid}. 

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 793 & 233 \\\hline 
      \textbf{Satisfied} & 191 & 562 \\\hline\hline
      \textbf{Accuracy} & 76.17\% & \\
      \hline
    \end{tabular}
    \caption{Confusion Matrix Training Data}
    \label{table:gcn_results_train}
  \end{table}

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 1'866 & 531 \\\hline 
      \textbf{Satisfied} & 440 & 1'384 \\\hline\hline
      \textbf{Accuracy} & 77.00\% & \\
      \hline
    \end{tabular}
    \caption{Confusion Matrix Validation Data}
    \label{table:gcn_results_valid}
  \end{table}

  \noindent Graph convolutional networks are designed to be used in a
  transductive setting and the model cannot be applied to new and unseen
  graphs. In this setting 30\% of the dataset was used for training and 70\% of
  the data was used for validation. This is done by masking the nodes in the
  graph which are part of the validations dataset. The masked nodes are 
  considered in the in the neighborhood function $\mathcal{N}(v)$, they are 
  however not considered as target nodes. Therefore, masked nodes are not used
  for updating the model parameters. The fact, that GCNs cannot be used in an
  inductive setting is very limiting. Further, the GCN requires 1'000
  epochs to finish training and yields only mediocre results in terms of
  accuracy and model fit. A reason for this could be, that only a full-batch
  implementation for GCN was introduced. In addition, GCNs always consider the
  entire neighborhood set. GraphSage can be considered as the inductive counter
  part which was specifically designed with mini-batch training in mind and
  makes use of neighborhood sampling. The results for GraphSage are presented
  in the following section. 

  \subsection{GraphSage}

  For GraphSage the exact same data input was used as for the GCN. The 
  GraphSage model included 2 convolutional layers with a hidden layer size of
  24 and an output layer size of 2. The model is defined using an adaptation of 
  algorithm \ref{algo:GraphSage}. Specifically, the hidden layer is activated 
  using the ReLU function with subsequent L2-normalization. The output of the 
  second layer is activated using the logSoftmax function. Normalization is 
  skipped for the final output. The training data was split into 80\% training 
  and 20\% validation using node masking. The loss is calculated using 
  cross-entropy and the model parameters are updated using the Adam optimizer 
  with the learning rate set to 0.002. The GraphSage model employed is thus an
  adaptation of the algorithm shown in algorithm \ref{algo:GNN_struct}. The 
  model was trained using a mini-batch size of 50 nodes and the neighborhood 
  function $\mathcal{N}(v)$ randomly sampled 10 neighbors 2-hops away from the 
  target node and randomly sampled 5 nodes at a 1-hop distance from the target 
  node. This corresponds to the steps shown in figure \ref{fig:GraphSage_sample}. 
  To improve the robustness of the model, lastly a dropout rate of $p = 0.02$ 
  was set. GraphSage was run using the different aggregation strategies mean, 
  LSTM, max-pooling. In addition, sum-pooling was added as an additional 
  aggregation strategy as suggested by \cite{xu2018powerful}. Sum-pooling 
  aggregation follows the same procedure as max-pooling with the only difference 
  being that element-wise sum-pooling is performed and not max-pooling. This 
  aggregation method has been shown to be more robust for graph neural networks, 
  as it allows the GNN to better distinguish different graph structures. It is
  important to note, that the Graphsage sum-pooling strategy employed does not
  correspond to the Graph Isomorphism Network introduced by 
  \cite{xu2018powerful}. This article rather provided the inspiration for
  applying sum-pooling for Graphsage. \\
  
  \noindent The model for each aggregation strategy was trained using 400 epochs. 
  The training- and validation results using the training graph are presented 
  for every aggregation strategy. In addition, the trained models are applied 
  to a new unseen graph also consisting of 6'000 nodes to check for the 
  inductive capability of the Graphsage model. 

  \paragraph{Mean Aggregation}  \mbox{}\\ 
  In figure \ref{fig:mean_aggregation} the training- and validation loss as
  well as the accuracies for the mean aggregation model is shown.

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_mean_plots.png}
		\caption{Mean Aggregation Loss- and Accuracy Plots}
        \label{fig:mean_aggregation}
  \end{figure}

  \noindent The training accuracy is 94.91\% and the validation accuracy is 
  94.80\% after 400 epochs. The model resulted in a test accuracy of 94.08\%
  with the following confusion matrix shown in table
  \ref{table:mean_results_test}:

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'278  & 88 \\\hline 
      \textbf{Satisfied} & 267 & 2'367 \\\hline\hline
      \textbf{Accuracy} & 94.08\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix Mean Aggregation}
    \label{table:mean_results_test}
  \end{table}

  \paragraph{LSTM Aggregation}  \mbox{}\\ 
  Figure \ref{fig:lstm_aggregation} shows the training- and validation loss
  and accuracy using LSTM aggregation. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_lstm_plots.png}
		\caption{LSTM Aggregation Loss- and Accuracy Plots}
        \label{fig:lstm_aggregation}
  \end{figure}

  \noindent The training- and validation accuracy after 400 epochs was 95.75\% 
  and 95.14\% respectively. Here again, we can see that the training behavior
  is relatively good with a small over-fit which remains stable. The results
  for the test graph are shown in table \ref{table:lstm_results_test}.

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'288  & 78 \\\hline 
      \textbf{Satisfied} & 268 & 2'366 \\\hline\hline
      \textbf{Accuracy} & 94.23\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix LSTM Aggregation}
    \label{table:lstm_results_test}
  \end{table}

  \paragraph{Sum-Pooling Aggregation}  \mbox{}\\ 
  Sum-pooling is shown to provide more consistent results as it prevents the
  GNN from being confused \citep{xu2018powerful}. This aggregation method 
  provides solid results as shown in figure \ref{fig:sum_aggregation}. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_sum_plots.png}
		\caption{Sum-Pooling Aggregation Loss- and Accuracy Plots}
        \label{fig:sum_aggregation}
  \end{figure}

  \noindent The plots show that sum-pooling trains very well. The training 
  accuracy is 94.60\% and validation accuracy is 94.97\% after the model
  finished training. The results for the test graph are shown in table
  \ref{table:sum_results_test}.

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'235  & 131 \\\hline 
      \textbf{Satisfied} & 213 & 2'421 \\\hline\hline
      \textbf{Accuracy} & 94.27\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix Sum-Pooling}
    \label{table:sum_results_test}
  \end{table}

  \paragraph{Max-Pooling Aggregation}  \mbox{}\\ 
  The last aggregation method is max-pooling which yielded similar results as
  sum-pooling. The training- and validation loss as well as the accuracies are 
  shown in figure \ref{fig:max_aggregation}. 

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{graphsage_max_plots.png}
		\caption{Max-Pooling Aggregation Loss- and Accuracy Plots}
        \label{fig:max_aggregation}
  \end{figure}

  \noindent The training model resulted with a training accuracy
  of 94.43\% and validation accuracy of 94.88\%. The results for the test graph
  are shown in table \ref{table:max_results_test}.

  \begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      \diagbox{\textbf{Label}}{\textbf{Predicted}} & \textbf{Neutral or
      Dissatisfied} & \textbf{Satisfied}\\
      \hline
      \textbf{Neutral or Dissatisfied} & 3'235  & 131 \\\hline 
      \textbf{Satisfied} & 212 & 2'422 \\\hline\hline
      \textbf{Accuracy} & 94.28\% & \\
      \hline
    \end{tabular}
    \caption{Test Confusion Matrix Max-Pooling}
    \label{table:max_results_test}
  \end{table}

  \subsection{GraphSage Robustness Simulation}
  
  The loss- and accuracy plots reveal mostly a relatively good model fit.
  Repeatedly training the GraphSage models revealed, that models would either
  over-fit, fit perfectly or have a training pattern in which the validation
  loss was lower than the training loss. An analysis revealed, that the culprit 
  for this behavior was the dropout rate of 2\% as well as the random assignment 
  of graph nodes into training- and validation data. If the dropout rate was set
  to 0, the model tended to over-fit, where the training accuracy would approach 
  100\% and the validation accuracy would stagnate around 93-94\%.
  Unfortunately, when applying the model with no dropout rate to an unseen test 
  graph, the accuracy results were somewhat lower at 91-92\%. For this reason, 
  a dropout rate of 2\% was set to avoid this over-fitting problem which in turn 
  yielded better test results. This has the consequence, that it is more 
  difficult for the model to make predictions for the training data compared to 
  the validation data which uses no dropout rate. This obstacle has the downside, 
  that it makes the model sensitive to the node assignments during the 
  random train- and validation split. Some nodes are more difficult to learn 
  depending on their features and neighbors. This means, that if the training set
  includes more difficult nodes on average than the validation set in
  conjunction with the dropout rate as an additional obstacle for the training
  set, the validation loss becomes lower than the training loss. The same is
  true in reverse, which leads to an over-fit. Lastly, the if the nodes in both
  the training- and validation set are on average of similar difficulty a good
  model fit is achieved as shown in the loss- and accuracy plots. \\

  \noindent Nevertheless, the training- and validation results shown are 
  representative for the model in general. In order to show this, a simulation 
  was run using a training- and validation graph consisting of 6'000 nodes and 
  a test graph also consisting of 6'000 nodes. For the simulation, max-pooling 
  aggregation was selected using the same model specifications as previously. 
  The model was trained during 100 experiments, where for every experiment a 
  new random training- and validation split node assignment was performed. The
  trained model was then applied to the test graph where the loss- and accuracy
  was measured. The average results of the 100 experiments are shown in table
  \ref{table:simulation_results} and figure \ref{fig:simulation_results}. 

  \begin{table}[h]
    \centering
      \begin{tabular}{|l||c|c|c|}
      \hline
      \textbf{Metric} & \textbf{Training Set} & \textbf{Validation Set} & 
      \textbf{Test Set}\\
      \hline\hline
      Average Accuracy & 95.01\% & 94.82\% & 93.80\% \\\hline 
                       & (0.36\%) & (0.71\%) & (0.47\%) \\\hline
      Average Cross-Entropy Loss & 0.3684 & 0.3693 & 0.3792 \\\hline
                                 & (0.0078) & (0.0182) & (0.0151) \\
      \hline
    \end{tabular}
    \caption{Average Simulation Results}
    \label{table:simulation_results}
  \end{table}

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{max_100_sim.png}
		\caption{Simulation Results Max-Pooling}
        \label{fig:simulation_results}
  \end{figure}

  \noindent Max-pooling was selected as this corresponds to the recommended
  aggregation strategy \citep[p. 9]{hamilton2017inductive}. For the remaining
  aggregation strategies, only 10 experiments were run due to time
  considerations as 100 experiments require approximately 15 hours to complete.
  The results using only 10 experiments yielded similar results, with test
  accuracies ranging predominantly from 93-94\%. 

  \section{Result Comparison}
  \label{section:result_comp}

  The results shown for the three models Node2Vec, GCN and GraphSage are
  compared to the results using "standard" machine learning methods. This is
  done to assess to what extent, synthetic graph generation is a useful
  approach for machine learning. This comparison is an important ingredient for
  answering the research question. These methods only consider the
  feature data and do not consider any network connections. These methods are
  therefore simpler and do not require a graph to be generated. A comparison of
  the different results is shown in table \ref{table:result_comparison}.

  \begin{table}[h]
    \centering
    \scalebox{0.75}{
      \begin{tabular}{|l||c|c|c|}
      \hline
      \textbf{Method} & \textbf{Training Accuracy} & \textbf{Validation
      Accuracy} & \textbf{Test Accuracy}\\
      \hline\hline
      Logistic Regression & 87.94\% & 87.17\% & 86.77\% \\\hline 
      Naive Bayes & 87.50\% & 86.17\% & 85.82\% \\\hline
      QDA & 70.5\% & 70.00\% & 69.23\% \\\hline
      AdaBoost & 94.06\% & 92.33\% & 93.00\% \\\hline
      Random Forest & 100\% & 94.42\% & 94.55\% \\\hline
      SVM & 94.65\% & 92.25\% & 92.73\% \\\hline
      ANN & 96.51\% & 94.58\% & 93.48\%\\\hline
      Node2Vec (Logistic Regression) & 77.04\% & 76.16\% & 57.06\% \\\hline
      GCN & 76.17\% & 77.00\% & -\\\hline
      GraphSage (Mean Aggregation) & 94.91\% & 94.80\% & 94.08\% \\\hline
      GraphSage (LSTM Aggregation) & 95.75\% & 95.14\% & 94.23\% \\\hline
      GraphSage (Sum-Pooling) & 94.60\% & 94.97\% & 94.27\% \\\hline
      GraphSage (Max-Pooling) & 94.43\% & 94.88\% & 94.28\% \\
      \hline
      \end{tabular}}
    \caption{Result Comparison}
    \label{table:result_comparison}
  \end{table}

  \noindent Table \ref{table:result_comparison} shows, that GCN and Node2Vec
  are not competitive when using MAG generated graphs. GraphSage is however a 
  serious competitor and appears to be the second best method overall. The
  Random Forest classifier is consistently the best method regardless of the
  training- and validation data as well as the test data used. The GraphSage
  models are consistently shown to be the second best method. The ANN model
  performs similarly well in terms of achieving a similar test accuracy.
  Similar as with the GNNs, it is important to evaluate the model fit to draw a
  more definitive conclusion which is shown in figure \ref{fig:ANN_fit}.

  \begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{ANN_Plot.png}
		\caption{ANN Model Fit}
        \label{fig:ANN_fit}
  \end{figure}

  \noindent Figure \ref{fig:ANN_fit} shows, that the ANN model has a relatively
  strong over-fit compared to the GNN models. This observation is consistent
  for every ANN model trained. In addition, different number of hidden layers, 
  hidden layer sizes and dropout rates were tested, which all resulted in a 
  clear over-fit or bad training behavior. For that reason, the ANN is deemed
  to be inferior to the GNN, even if the test accuracies are very similar. \\

  \noindent AdaBoost and SVM also are shown to provide very good results,
  however yield slightly lower test accuracies compared to the previous
  methods. Lastly, naive bayes and logistic regression clearly yield inferior
  results. 
